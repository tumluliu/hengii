// This autogenerated skeleton file illustrates how to build a server.
// You should copy it to another filename to avoid overwriting it.

#include "HpgcJob.h"
#include <iostream>
#include <fstream>
#include <concurrency/ThreadManager.h>
#include <concurrency/PosixThreadFactory.h>
#include <protocol/TBinaryProtocol.h>
#include <server/TSimpleServer.h>
#include <server/TNonblockingServer.h>
#include <transport/TServerSocket.h>
#include <transport/TBufferTransports.h>
#include <string>
extern "C"
{
#include <pbs_error.h>
#include <pbs_ifl.h>
#include <unistd.h>
#include <signal.h>
#include <sys/param.h>
#include <sys/types.h>
#include <sys/stat.h>
}

using namespace ::apache::thrift;
using namespace ::apache::thrift::protocol;
using namespace ::apache::thrift::transport;
using namespace ::apache::thrift::server;
using namespace ::apache::thrift::concurrency;
		
using boost::shared_ptr;

using namespace higis;
using namespace std;

//#define BUFFER_SIZE 10240
#define PARAM_SIZE 1024

	  int FindNum(char *point);
	  void* threadFunction(void* Param);
	  void init_daemon(void);
	  

	 
 class ThreadParam{
		  public:
		  struct Task task;
		  pthread_mutex_t* state_mutex_GLOBAL;
		  pthread_cond_t* state_threshold_cv_GLOBAL;
		  int* state_GLOBAL;
		  string result;
	  };
	  
	  class HpgcJobHandler : virtual public HpgcJobIf {
 public:
		  
  HpgcJobHandler() {
    // Your initialization goes here
  }

  void run(JobResult& _return, const Job& job);

  double get_progress(const Job& job) {
    // Your implementation goes here
    printf("get_progress\n");
	return 0;
  }
  
};

 void HpgcJobHandler::run(JobResult& _return, const Job& job) {
    // Your implementation goes here
		cout<<"Start running HPGC job..."<<endl;
		cout<<"number of tasks in this job: "<<job.task_count<<endl;
	
		 pthread_t threadId_GLOBAL[job.task_count];
	 	 pthread_mutex_t state_mutex_global;
		 pthread_cond_t state_threshold_cv_global;
		pthread_attr_t attr;
		
		 pthread_mutex_init(&state_mutex_global, NULL);
		 pthread_cond_init (&state_threshold_cv_global, NULL);
		 pthread_attr_init(&attr);
		 pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE);
		int state_global[job.task_count];
		for(int i=0;i<job.task_count;i++)
		{
			state_global[i]=job.tasks[i].parent_count;
		}
		ThreadParam threadparam[job.task_count];
		for(int i=0;i<job.task_count;i++)
		{
			threadparam[i].state_GLOBAL=state_global;
			threadparam[i].state_mutex_GLOBAL=&state_mutex_global;
			threadparam[i].state_threshold_cv_GLOBAL=&state_threshold_cv_global;
			threadparam[i].task=job.tasks[i];
			pthread_create(&threadId_GLOBAL[i], &attr, threadFunction, &(threadparam[i]));
		}
		string senddata;
	  for (int i=0; i<job.task_count; i++) 
		{	
	    pthread_join(threadId_GLOBAL[i], NULL);
		}
		for(int i=0;i<job.task_count;i++)
		{
		senddata+=threadparam[i].result;
		}
		char Path[PARAM_SIZE];
		memset(Path, '\0', sizeof(Path) ); 
		getcwd(Path,sizeof(Path));
		string run_path=Path;
 		run_path+="/run.log";
		ofstream outfile(run_path.c_str(),ios::out);
		if (!outfile)
			cout<<"open run.log error!"<<endl;
		outfile<<senddata<<endl;
		outfile.close();
		cout<<"HPGC job finished."<<endl;
	// put the job execution codes here
	_return.status = 0;
//	_return.result = "task1 and task2 are both finished successfully.";
	_return.result=senddata;

  pthread_attr_destroy(&attr);
  pthread_mutex_destroy(&state_mutex_global);
  pthread_cond_destroy(&state_threshold_cv_global);
//  pthread_exit(NULL);
  }


string Read(string file)
{
	string content;
	ifstream infile(file.c_str(),ios::in);
	if (!infile)
		cout<<"open file error!"<<endl;
	content.clear();
	copy(istreambuf_iterator<char>(infile),istreambuf_iterator<char>(),back_inserter(content));
	if (isspace(content[content.size()-1]))	
		content[content.size()-1]=' ';
	infile.close();
//	cout<<content<<endl;
	return content;
}

void WriteToLog(char *file,char *str)
{
	FILE *fp;
	time_t t;
	fp=fopen(file,"a");
	if((fp) >=0)
	 {
	 	 t=time(0);
		 fprintf(fp,"%s:%s\n",asctime(localtime(&t)),str );
		 fclose(fp);
	  }
}

 void* threadFunction(void* Param)
	 {
	 struct ThreadParam* params=(struct ThreadParam*) Param;
	 struct Task* task = &params->task;
	 	string logBuf;
		string cmdline;
		string sendData;
//	 	char str1[]="$";
	 	int taskID,parent_count,childcount;
	 	int rnd,procs,i;
//	 	FILE *Tfp,*fp;
	   char script[11];
	   char stdout[]="pbs";
		char error[]="error.log";
		char dir[]="/";
		char scrPath[PARAM_SIZE];
	   char stdPath[PARAM_SIZE];
	   char errPath[PARAM_SIZE];
		char Path[PARAM_SIZE];
		char Hostname[PARAM_SIZE];

	  	memset(scrPath, '\0', sizeof(scrPath) ); 
	   memset(stdPath, '\0', sizeof(stdPath) );  
		memset(errPath, '\0', sizeof(errPath) );
		memset(script, '\0', sizeof(script) );
		memset(Path, '\0', sizeof(Path) ); 
	   memset(Hostname, '\0', sizeof(Hostname) );  

		getcwd(Path,sizeof(Path));
		strcat(Path,dir);
		gethostname(Hostname,sizeof(Hostname));
	 	taskID=task->id;
	 	parent_count=task->parent_count;
		childcount=task->child_count;
		int child[childcount];
		for(i=0;i<childcount;i++)
	 		{
			child[i]=task->children[i];
	 		}
		procs=task->process_count;
		char proc[2];
		sprintf(proc,"%d",procs);
		string conf_path=Path;
 		conf_path+="hpgcserver.conf";
		if(procs==1)
			cmdline=task->program_name;
		else
		{
			cmdline="mpiexec -n ";
			cmdline=cmdline+proc+" "+task->mpi_options+" "+Read(conf_path)+" "+task->program_name;
		}
		for(i=0;i<task->program_param_count;i++)
			cmdline=cmdline+" "+task->program_params[i];
		cout<<"cmdline:"<<cmdline<<endl;

/*	    if(params->state_GLOBAL[taskID]<0)
	 	{
	      WriteToLog("error.log","error reliance!");
	 	    sendData="Run failed:error reliance!";
	 	    goto loop;
	 	}
*/
	 	pthread_mutex_lock(params->state_mutex_GLOBAL);//加锁
	 	while(*(params->state_GLOBAL+taskID)!=0)//等到parent_count==0
	 	pthread_cond_wait(params->state_threshold_cv_GLOBAL, params->state_mutex_GLOBAL);//等待
	 	pthread_mutex_unlock(params->state_mutex_GLOBAL);//解锁

	 	//执行计算作业====================================================================================
		struct timeval tpstart;
		gettimeofday(&tpstart,NULL);
		srand(tpstart.tv_usec);
		rnd=rand();
		sprintf(script,"%d",rnd);

		strcat(scrPath,Path);
		strcat(scrPath,script);
		strcat(stdPath,Path);
		strcat(stdPath,stdout);
		strcat(stdPath,script);
		strcat(errPath,Path);
		strcat(errPath,error);
//		cout<<"scrPath:"<<scrPath<<endl;
//		cout<<"stdPath:"<<stdPath<<endl;
//		cout<<"errPath:"<<errPath<<endl;
		int exist;
		char str3[]="0";
		while((exist=access(scrPath,0))==0)
	  	   {
			printf("%s exist!\n",scrPath);
			strcat(script,str3);
			strcat(scrPath,str3);
			strcat(stdPath,str3);	    	
	  	   }
		ofstream outfile(scrPath,ios::out);
		if (!outfile)
			cout<<"open file error!"<<endl;
		outfile<<cmdline<<endl;
		outfile.close();

	 	int Con=pbs_connect(Hostname); //连结服务器”server”

		struct attropl attrib[4];

		attrib[0].name = ATTR_N;
		attrib[0].value = script;
		attrib[0].resource='\0';
		attrib[0].next=&attrib[1];
		attrib[1].name=ATTR_o;
		attrib[1].value=stdPath;
		attrib[1].resource='\0';
		attrib[1].next=&attrib[2];
		attrib[2].name=ATTR_e;
		attrib[2].value=errPath;
		attrib[2].resource='\0';
		attrib[2].next=&attrib[3];
		attrib[3].name=ATTR_l;
		attrib[3].value=proc;
		attrib[3].resource="procs";
		attrib[3].next=NULL;
		
		char *Ret=pbs_submit(Con,attrib,scrPath,0,0);//提交任务
		if (!Ret)
		{
			printf("ERR=%d\n",pbs_errno);//发生错误，打印错误代码
			printf( "error[%s]", strerror( pbs_errno ) );
		}
		else
		{
			cout<<"Ret="<<Ret<<endl;//打印返回的任务标识
		}

		struct attrl attr;
		attr.name = ATTR_state;
		attr.value = NULL;
		attr.resource=NULL;
		attr.next=NULL;

		char *state;
		struct batch_status *status;
		do
		{
		  status=pbs_statjob(Con,Ret,&attr,0);
		  state=status->attribs->value;
		}while(*state!='C');
		logBuf=Read(stdPath);
		sendData=logBuf;
//		 sendData+=str1;
	 	//发送计算结果消息
	 	//endof执行计算作业====================================================================================
	 	char del[PARAM_SIZE]="rm ";
	 	char str2[]=" ";
	 	strcat(del,scrPath);
	 	strcat(del,str2);
	 	strcat(del,stdPath);
//	 	cout<<"del:"<<del<<endl;
	 	int result=system(del);
	/*   if(result!=0)
	        {
		}*/
		free(Ret);
		pbs_statfree(status);
		pbs_disconnect(Con);
		
	 	pthread_mutex_lock(params->state_mutex_GLOBAL);//加锁
		for(i=0;i<childcount;i++)
	 	{
			int childID=child[i];
			(*(params->state_GLOBAL+childID))--;//将child的parent_count减1
	 	}
	 	pthread_mutex_unlock(params->state_mutex_GLOBAL);//解锁
	 	pthread_cond_broadcast(params->state_threshold_cv_GLOBAL);//广播

//		loop:params->result[taskID].clear();
		params->result.clear();
		params->result=sendData;
		cout<<"The sendData is:"<<	params->result<<endl;
		pthread_exit(NULL);
 }

 void init_daemon(void)
 {
 	pid_t pid;
 	int i;
 	if((pid=fork()))
 	exit(0);//是父进程，结束父进程
 	else if(pid< 0)
 	{
  		WriteToLog("error.log","create deamon error!!");
 		exit(1);//fork失败，退出
 	}
 	setsid();
 	if((pid=fork()))
 	exit(0);//是第一子进程，结束第一子进程
 	else if(pid< 0)
 	{
  		WriteToLog("error.log","create deamon error!!");
 		exit(1);//fork失败，退出
 	}

 	for(i=0;i< NOFILE;++i)//关闭打开的文件描述符
 	close(i);
 	umask(0);//重设文件创建掩模
 	return;
 }
 
int main(int argc, char **argv) {
  int port = 4742;
  int workerCount = 16; 
//	init_daemon();
  shared_ptr<HpgcJobHandler> handler(new HpgcJobHandler());
  shared_ptr<TProcessor> processor(new HpgcJobProcessor(handler));
  shared_ptr<TProtocolFactory> protocolFactory(new TBinaryProtocolFactory());
  shared_ptr<ThreadManager> threadManager = ThreadManager::newSimpleThreadManager(workerCount);
  shared_ptr<PosixThreadFactory> threadFactory = shared_ptr<PosixThreadFactory>(new PosixThreadFactory());
  threadManager->threadFactory(threadFactory);
  threadManager->start();
  TNonblockingServer server(processor, protocolFactory, port, threadManager);
//  TNonblockingServer server(processor,protocolFactory,port);
  cout<<"Starting hpgcserver..."<<endl;

  server.serve();
  return 0;
}


